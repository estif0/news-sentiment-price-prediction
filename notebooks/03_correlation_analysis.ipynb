{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ef7ae8",
   "metadata": {},
   "source": [
    "# Task 3: Correlation Analysis - News Sentiment vs Stock Returns\n",
    "\n",
    "**Objective**: Establish quantifiable correlations between financial news sentiment and stock price movements to develop actionable investment insights for Nova Financial Solutions.\n",
    "\n",
    "## Business Context\n",
    "\n",
    "This analysis addresses the fundamental question: **Can news sentiment predict stock price movements?** By correlating sentiment polarity scores from 1.4M+ financial news headlines with daily stock returns across 6 major technology companies, we aim to:\n",
    "\n",
    "1. **Validate Predictive Power**: Determine if sentiment has statistically significant correlation with returns\n",
    "2. **Identify Lag Effects**: Test if today's sentiment predicts tomorrow's price movements (T+1, T+2)\n",
    "3. **Quantify Relationships**: Calculate Pearson correlation coefficients and p-values for confidence assessment\n",
    "4. **Generate Trading Signals**: Develop actionable recommendations based on sentiment-return patterns\n",
    "\n",
    "**Expected Outcomes**:\n",
    "- Correlation coefficients (target: >0.3 for meaningful relationships)\n",
    "- Statistical significance (p-value < 0.05 for confidence)\n",
    "- Predictive lag analysis for algorithmic trading strategies\n",
    "- Risk-adjusted investment recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035eca7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import core data processing, NLP, statistical analysis, and custom modules for sentiment-price correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90775ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "ğŸ“Š pandas version: 2.3.3\n",
      "ğŸ”¢ numpy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Configure plotting\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add src to path for custom modules\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.core.data_loader import DataLoader\n",
    "from src.core.data_processor import DataProcessor\n",
    "from src.core.sentiment_analyzer import SentimentAnalyzer\n",
    "from src.core.financial_analyzer import FinancialAnalyzer\n",
    "from src.core.visualizer import Visualizer\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ğŸ“Š pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbc3ed",
   "metadata": {},
   "source": [
    "## 2. Load News and Stock Data\n",
    "\n",
    "Load 1.4M+ financial news articles and historical stock price data (OHLCV) for correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea64892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“° Loading financial news data...\n",
      "âœ… Loaded 1,407,328 news articles\n",
      "âœ… Loaded 1,407,328 news articles\n",
      "   Date range: 2009-02-14 00:00:00 to 2020-06-11 17:12:35-04:00\n",
      "   Unique stocks: 6204\n",
      "   Date range: 2009-02-14 00:00:00 to 2020-06-11 17:12:35-04:00\n",
      "   Unique stocks: 6204\n",
      "   Unique publishers: 1034\n",
      "\n",
      "ğŸ“ˆ Loading stock price data for 6 symbols...\n",
      "   Unique publishers: 1034\n",
      "\n",
      "ğŸ“ˆ Loading stock price data for 6 symbols...\n",
      "   âœ… AAPL: 3774 trading days\n",
      "   âœ… AMZN: 3774 trading days\n",
      "   âœ… GOOG: 3774 trading days\n",
      "   âœ… META: 2923 trading days\n",
      "   âœ… MSFT: 3774 trading days\n",
      "   âœ… NVDA: 3774 trading days\n",
      "\n",
      "âœ… Successfully loaded 6 stock datasets\n",
      "   âœ… AAPL: 3774 trading days\n",
      "   âœ… AMZN: 3774 trading days\n",
      "   âœ… GOOG: 3774 trading days\n",
      "   âœ… META: 2923 trading days\n",
      "   âœ… MSFT: 3774 trading days\n",
      "   âœ… NVDA: 3774 trading days\n",
      "\n",
      "âœ… Successfully loaded 6 stock datasets\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "loader = DataLoader()\n",
    "\n",
    "# Load news data\n",
    "print(\"ğŸ“° Loading financial news data...\")\n",
    "news_df = loader.load_news_data(\"../data/raw/raw_analyst_ratings.csv\")\n",
    "print(f\"âœ… Loaded {len(news_df):,} news articles\")\n",
    "print(f\"   Date range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "print(f\"   Unique stocks: {news_df['stock'].nunique()}\")\n",
    "print(f\"   Unique publishers: {news_df['publisher'].nunique()}\")\n",
    "\n",
    "# Load stock data for all 6 symbols\n",
    "stock_symbols = [\"AAPL\", \"AMZN\", \"GOOG\", \"META\", \"MSFT\", \"NVDA\"]\n",
    "stock_data = {}\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Loading stock price data for {len(stock_symbols)} symbols...\")\n",
    "for symbol in stock_symbols:\n",
    "    try:\n",
    "        stock_df = loader.load_stock_data(f\"../data/cleaned/{symbol}.csv\")\n",
    "        stock_data[symbol] = stock_df\n",
    "        print(f\"   âœ… {symbol}: {len(stock_df)} trading days\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {symbol}: Error - {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully loaded {len(stock_data)} stock datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e333a",
   "metadata": {},
   "source": [
    "## 3. Date Normalization and Alignment\n",
    "\n",
    "Convert UTC-4 news timestamps to trading days, mapping weekend/holiday news to the next trading day (Monday) for proper alignment with stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9872aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Module reloaded\n"
     ]
    }
   ],
   "source": [
    "# Reload modules to pick up fixes\n",
    "import importlib\n",
    "import src.core.data_processor\n",
    "\n",
    "importlib.reload(src.core.data_processor)\n",
    "from src.core.data_processor import DataProcessor\n",
    "\n",
    "processor = DataProcessor()\n",
    "print(\"âœ… Module reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4efd787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ• Normalizing news dates to trading days...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Normalize news dates to trading days\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ• Normalizing news dates to trading days...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m news_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display normalization results\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… News dates normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/ai-ml/news-sentiment-price-prediction/src/core/data_processor.py:46\u001b[0m, in \u001b[0;36mDataProcessor.normalize_dates\u001b[0;34m(self, news_df, date_column)\u001b[0m\n\u001b[1;32m     43\u001b[0m     df[date_column] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[date_column], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Now handle timezone conversion\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39mtz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Timezone-naive - assume US/Eastern (UTC-4)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_eastern\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[date_column]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meastern_tz)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Already has timezone, convert to Eastern\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ai-ml/news-sentiment-price-prediction/venv/lib/python3.10/site-packages/pandas/core/generic.py:6321\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6315\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6316\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6317\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6319\u001b[0m ):\n\u001b[1;32m   6320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ai-ml/news-sentiment-price-prediction/venv/lib/python3.10/site-packages/pandas/core/accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/Projects/ai-ml/news-sentiment-price-prediction/venv/lib/python3.10/site-packages/pandas/core/indexes/accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Normalize news dates to trading days\n",
    "print(\"ğŸ• Normalizing news dates to trading days...\")\n",
    "news_normalized = processor.normalize_dates(news_df, date_column=\"date\")\n",
    "\n",
    "# Display normalization results\n",
    "print(f\"âœ… News dates normalized\")\n",
    "print(f\"   Original date range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "print(\n",
    "    f\"   Trading date range: {news_normalized['trading_date'].min()} to {news_normalized['trading_date'].max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Weekend articles remapped: {(news_normalized['trading_date'].dt.dayofweek >= 5).sum():,}\"\n",
    ")\n",
    "\n",
    "# Check for duplicate dates (multiple articles same day/stock)\n",
    "duplicates = news_normalized.groupby([\"trading_date\", \"stock\"]).size()\n",
    "print(f\"   Articles per day (avg): {duplicates.mean():.2f}\")\n",
    "print(f\"   Max articles same day/stock: {duplicates.max()}\")\n",
    "\n",
    "# Preview normalized data\n",
    "news_normalized[[\"headline\", \"stock\", \"trading_date\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c99e73",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis on Headlines\n",
    "\n",
    "Apply TextBlob sentiment analysis to extract polarity scores (-1 to +1) from news headlines, classifying sentiment as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Analyze all headlines for sentiment\n",
    "print(\"ğŸ’­ Analyzing sentiment for all headlines...\")\n",
    "print(f\"   Total headlines to process: {len(news_normalized):,}\")\n",
    "\n",
    "# Batch sentiment analysis (efficient for large datasets)\n",
    "news_with_sentiment = sentiment_analyzer.analyze_dataframe(\n",
    "    news_normalized, text_column=\"headline\"\n",
    ")\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(f\"\\nâœ… Sentiment analysis complete!\")\n",
    "print(f\"\\nğŸ“Š Sentiment Distribution:\")\n",
    "print(news_with_sentiment[\"sentiment_category\"].value_counts())\n",
    "\n",
    "sentiment_stats = news_with_sentiment[\"sentiment_polarity\"].describe()\n",
    "print(f\"\\nğŸ“ˆ Sentiment Polarity Statistics:\")\n",
    "print(f\"   Mean: {sentiment_stats['mean']:.4f}\")\n",
    "print(f\"   Median: {sentiment_stats['50%']:.4f}\")\n",
    "print(f\"   Std Dev: {sentiment_stats['std']:.4f}\")\n",
    "print(f\"   Range: [{sentiment_stats['min']:.4f}, {sentiment_stats['max']:.4f}]\")\n",
    "\n",
    "# Preview sentiment scores\n",
    "news_with_sentiment[\n",
    "    [\"headline\", \"stock\", \"sentiment_polarity\", \"sentiment_category\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2451c",
   "metadata": {},
   "source": [
    "### Visualize Sentiment Distribution\n",
    "\n",
    "Examine the distribution of sentiment scores across all headlines to understand the sentiment landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment distribution visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram of sentiment polarity scores\n",
    "axes[0].hist(\n",
    "    news_with_sentiment[\"sentiment_polarity\"], bins=50, edgecolor=\"black\", alpha=0.7\n",
    ")\n",
    "axes[0].axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Neutral (0)\")\n",
    "axes[0].set_xlabel(\"Sentiment Polarity Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    \"Distribution of Sentiment Polarity Scores\\n(All Headlines)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart of sentiment categories\n",
    "category_counts = news_with_sentiment[\"sentiment_category\"].value_counts()\n",
    "colors = {\"positive\": \"green\", \"negative\": \"red\", \"neutral\": \"gray\"}\n",
    "category_colors = [colors.get(cat, \"blue\") for cat in category_counts.index]\n",
    "\n",
    "axes[1].bar(\n",
    "    category_counts.index,\n",
    "    category_counts.values,\n",
    "    color=category_colors,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[1].set_xlabel(\"Sentiment Category\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Number of Headlines\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    \"Sentiment Classification Distribution\\n(Positive/Neutral/Negative)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (cat, count) in enumerate(category_counts.items()):\n",
    "    axes[1].text(\n",
    "        i,\n",
    "        count + len(news_with_sentiment) * 0.01,\n",
    "        f\"{count:,}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ“Š Sentiment Distribution Insights:\")\n",
    "print(\n",
    "    f\"   Positive: {(news_with_sentiment['sentiment_category'] == 'positive').sum():,} ({(news_with_sentiment['sentiment_category'] == 'positive').sum()/len(news_with_sentiment)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Neutral:  {(news_with_sentiment['sentiment_category'] == 'neutral').sum():,} ({(news_with_sentiment['sentiment_category'] == 'neutral').sum()/len(news_with_sentiment)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Negative: {(news_with_sentiment['sentiment_category'] == 'negative').sum():,} ({(news_with_sentiment['sentiment_category'] == 'negative').sum()/len(news_with_sentiment)*100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3382e",
   "metadata": {},
   "source": [
    "## 5. Calculate Daily Stock Returns\n",
    "\n",
    "Compute daily percentage returns for each stock using the formula: `(Close_today - Close_yesterday) / Close_yesterday * 100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize financial analyzer\n",
    "financial_analyzer = FinancialAnalyzer()\n",
    "\n",
    "# Calculate returns for all stocks\n",
    "print(\"ğŸ“ˆ Calculating daily returns for all stocks...\")\n",
    "stock_returns = {}\n",
    "\n",
    "for symbol, stock_df in stock_data.items():\n",
    "    # Calculate returns\n",
    "    returns_df = financial_analyzer.calculate_returns(stock_df.copy())\n",
    "\n",
    "    # Add symbol column for later merging\n",
    "    returns_df[\"stock\"] = symbol\n",
    "\n",
    "    # Rename Date to trading_date for alignment\n",
    "    if \"Date\" in returns_df.columns:\n",
    "        returns_df[\"trading_date\"] = pd.to_datetime(returns_df[\"Date\"])\n",
    "    else:\n",
    "        returns_df[\"trading_date\"] = pd.to_datetime(returns_df.index)\n",
    "\n",
    "    stock_returns[symbol] = returns_df\n",
    "\n",
    "    # Display statistics\n",
    "    returns_stats = returns_df[\"daily_return\"].describe()\n",
    "    print(f\"\\nâœ… {symbol}:\")\n",
    "    print(f\"   Mean return: {returns_stats['mean']:.4f}%\")\n",
    "    print(f\"   Volatility (std): {returns_stats['std']:.4f}%\")\n",
    "    print(f\"   Range: [{returns_stats['min']:.4f}%, {returns_stats['max']:.4f}%]\")\n",
    "\n",
    "# Combine all returns into single DataFrame\n",
    "all_returns = pd.concat(\n",
    "    [\n",
    "        df[[\"trading_date\", \"stock\", \"daily_return\", \"Close\"]]\n",
    "        for df in stock_returns.values()\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Combined Returns Dataset:\")\n",
    "print(f\"   Total observations: {len(all_returns):,}\")\n",
    "print(\n",
    "    f\"   Date range: {all_returns['trading_date'].min()} to {all_returns['trading_date'].max()}\"\n",
    ")\n",
    "print(f\"   Stocks: {all_returns['stock'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1fc4d4",
   "metadata": {},
   "source": [
    "## 6. Aggregate Daily Sentiment Scores\n",
    "\n",
    "Group sentiment scores by date and stock symbol to calculate daily average sentiment, handling multiple news articles per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily sentiment by stock\n",
    "print(\"ğŸ“Š Aggregating daily sentiment scores...\")\n",
    "daily_sentiment = sentiment_analyzer.aggregate_daily_sentiment(\n",
    "    news_with_sentiment, date_column=\"trading_date\"\n",
    ")\n",
    "\n",
    "# Display aggregation statistics\n",
    "print(f\"\\nâœ… Daily sentiment aggregated\")\n",
    "print(f\"   Total trading days with news: {len(daily_sentiment):,}\")\n",
    "print(\n",
    "    f\"   Date range: {daily_sentiment['trading_date'].min()} to {daily_sentiment['trading_date'].max()}\"\n",
    ")\n",
    "print(f\"   Stocks with daily sentiment: {daily_sentiment['stock'].nunique()}\")\n",
    "\n",
    "# Check aggregation statistics\n",
    "print(f\"\\nğŸ“ˆ Aggregation Statistics:\")\n",
    "print(f\"   Average articles per day: {daily_sentiment['article_count'].mean():.2f}\")\n",
    "print(f\"   Max articles same day: {daily_sentiment['article_count'].max()}\")\n",
    "print(f\"   Days with single article: {(daily_sentiment['article_count'] == 1).sum():,}\")\n",
    "print(f\"   Days with 5+ articles: {(daily_sentiment['article_count'] >= 5).sum():,}\")\n",
    "\n",
    "# Display sample of aggregated data\n",
    "print(f\"\\nğŸ“‹ Sample Daily Sentiment Data:\")\n",
    "daily_sentiment[\n",
    "    [\"trading_date\", \"stock\", \"avg_sentiment\", \"article_count\", \"positive_ratio\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00771a8e",
   "metadata": {},
   "source": [
    "## 7. Merge Sentiment and Returns Data\n",
    "\n",
    "Align daily sentiment scores with stock returns by date and symbol to create the unified dataset for correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentiment and returns on trading_date and stock\n",
    "print(\"ğŸ”— Merging sentiment and returns data...\")\n",
    "merged_data = pd.merge(\n",
    "    daily_sentiment, all_returns, on=[\"trading_date\", \"stock\"], how=\"inner\"\n",
    ")\n",
    "\n",
    "# Display merge results\n",
    "print(f\"\\nâœ… Data merged successfully\")\n",
    "print(f\"   Total matched observations: {len(merged_data):,}\")\n",
    "print(\n",
    "    f\"   Date range: {merged_data['trading_date'].min()} to {merged_data['trading_date'].max()}\"\n",
    ")\n",
    "print(f\"   Stocks in merged data: {merged_data['stock'].nunique()}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_sentiment = merged_data[\"avg_sentiment\"].isna().sum()\n",
    "missing_returns = merged_data[\"daily_return\"].isna().sum()\n",
    "print(f\"\\nğŸ” Data Quality:\")\n",
    "print(f\"   Missing sentiment scores: {missing_sentiment}\")\n",
    "print(f\"   Missing return values: {missing_returns}\")\n",
    "\n",
    "# Distribution by stock\n",
    "print(f\"\\nğŸ“Š Observations per Stock:\")\n",
    "for symbol in merged_data[\"stock\"].unique():\n",
    "    count = len(merged_data[merged_data[\"stock\"] == symbol])\n",
    "    print(f\"   {symbol}: {count:,} days\")\n",
    "\n",
    "# Preview merged data\n",
    "print(f\"\\nğŸ“‹ Sample Merged Data:\")\n",
    "merged_data[\n",
    "    [\"trading_date\", \"stock\", \"avg_sentiment\", \"daily_return\", \"article_count\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8ccc4",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis: Sentiment vs Returns\n",
    "\n",
    "Calculate Pearson correlation coefficients between daily sentiment scores and daily stock returns, with statistical significance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall correlation (all stocks combined)\n",
    "print(\"ğŸ“ˆ Calculating Sentiment-Return Correlations...\\n\")\n",
    "\n",
    "# Remove any NaN values for correlation calculation\n",
    "clean_data = merged_data.dropna(subset=[\"avg_sentiment\", \"daily_return\"])\n",
    "\n",
    "# Overall correlation\n",
    "overall_corr, overall_pval = pearsonr(\n",
    "    clean_data[\"avg_sentiment\"], clean_data[\"daily_return\"]\n",
    ")\n",
    "\n",
    "print(f\"ğŸŒ Overall Correlation (All Stocks Combined):\")\n",
    "print(f\"   Pearson Correlation: {overall_corr:.4f}\")\n",
    "print(f\"   P-value: {overall_pval:.6f}\")\n",
    "print(\n",
    "    f\"   Significance: {'âœ… Statistically Significant (p < 0.05)' if overall_pval < 0.05 else 'âŒ Not Significant (p >= 0.05)'}\"\n",
    ")\n",
    "print(f\"   Observations: {len(clean_data):,}\")\n",
    "\n",
    "# Per-stock correlations\n",
    "print(f\"\\nğŸ“Š Per-Stock Correlations:\")\n",
    "stock_correlations = []\n",
    "\n",
    "for symbol in sorted(clean_data[\"stock\"].unique()):\n",
    "    stock_data = clean_data[clean_data[\"stock\"] == symbol]\n",
    "\n",
    "    if len(stock_data) > 2:  # Need at least 3 points for correlation\n",
    "        corr, pval = pearsonr(stock_data[\"avg_sentiment\"], stock_data[\"daily_return\"])\n",
    "\n",
    "        stock_correlations.append(\n",
    "            {\n",
    "                \"stock\": symbol,\n",
    "                \"correlation\": corr,\n",
    "                \"p_value\": pval,\n",
    "                \"significant\": pval < 0.05,\n",
    "                \"observations\": len(stock_data),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        sig_icon = \"âœ…\" if pval < 0.05 else \"âŒ\"\n",
    "        print(\n",
    "            f\"   {symbol}: r={corr:+.4f}, p={pval:.6f} {sig_icon} (n={len(stock_data):,})\"\n",
    "        )\n",
    "\n",
    "# Create DataFrame for correlations\n",
    "correlations_df = pd.DataFrame(stock_correlations)\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\nğŸ“ˆ Correlation Statistics:\")\n",
    "print(f\"   Mean correlation: {correlations_df['correlation'].mean():.4f}\")\n",
    "print(f\"   Median correlation: {correlations_df['correlation'].median():.4f}\")\n",
    "print(f\"   Std deviation: {correlations_df['correlation'].std():.4f}\")\n",
    "print(\n",
    "    f\"   Range: [{correlations_df['correlation'].min():.4f}, {correlations_df['correlation'].max():.4f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"   Stocks with significant correlation: {correlations_df['significant'].sum()} / {len(correlations_df)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58931e4",
   "metadata": {},
   "source": [
    "## 9. Lagged Correlation Analysis\n",
    "\n",
    "Test if today's sentiment predicts tomorrow's returns (lag-1) and day-after returns (lag-2) to identify predictive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lagged correlations (sentiment today vs returns tomorrow/day-after)\n",
    "print(\"â° Calculating Lagged Correlations...\\n\")\n",
    "\n",
    "# Prepare data with lags by stock\n",
    "lagged_correlations = []\n",
    "\n",
    "for symbol in sorted(clean_data[\"stock\"].unique()):\n",
    "    stock_data = clean_data[clean_data[\"stock\"] == symbol].sort_values(\"trading_date\")\n",
    "\n",
    "    if len(stock_data) < 5:  # Need enough data for lagged analysis\n",
    "        continue\n",
    "\n",
    "    # Create lagged returns (shift forward)\n",
    "    stock_data = stock_data.copy()\n",
    "    stock_data[\"return_t1\"] = stock_data[\"daily_return\"].shift(-1)  # Tomorrow's return\n",
    "    stock_data[\"return_t2\"] = stock_data[\"daily_return\"].shift(-2)  # Day-after return\n",
    "\n",
    "    # Same-day correlation (T+0)\n",
    "    valid_t0 = stock_data.dropna(subset=[\"avg_sentiment\", \"daily_return\"])\n",
    "    if len(valid_t0) > 2:\n",
    "        corr_t0, pval_t0 = pearsonr(valid_t0[\"avg_sentiment\"], valid_t0[\"daily_return\"])\n",
    "    else:\n",
    "        corr_t0, pval_t0 = np.nan, np.nan\n",
    "\n",
    "    # Lag-1 correlation (T+1): sentiment today vs return tomorrow\n",
    "    valid_t1 = stock_data.dropna(subset=[\"avg_sentiment\", \"return_t1\"])\n",
    "    if len(valid_t1) > 2:\n",
    "        corr_t1, pval_t1 = pearsonr(valid_t1[\"avg_sentiment\"], valid_t1[\"return_t1\"])\n",
    "    else:\n",
    "        corr_t1, pval_t1 = np.nan, np.nan\n",
    "\n",
    "    # Lag-2 correlation (T+2): sentiment today vs return day-after\n",
    "    valid_t2 = stock_data.dropna(subset=[\"avg_sentiment\", \"return_t2\"])\n",
    "    if len(valid_t2) > 2:\n",
    "        corr_t2, pval_t2 = pearsonr(valid_t2[\"avg_sentiment\"], valid_t2[\"return_t2\"])\n",
    "    else:\n",
    "        corr_t2, pval_t2 = np.nan, np.nan\n",
    "\n",
    "    lagged_correlations.append(\n",
    "        {\n",
    "            \"stock\": symbol,\n",
    "            \"corr_t0\": corr_t0,\n",
    "            \"pval_t0\": pval_t0,\n",
    "            \"corr_t1\": corr_t1,\n",
    "            \"pval_t1\": pval_t1,\n",
    "            \"corr_t2\": corr_t2,\n",
    "            \"pval_t2\": pval_t2,\n",
    "            \"observations\": len(stock_data),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"{symbol}:\")\n",
    "    print(\n",
    "        f\"   T+0 (same day):  r={corr_t0:+.4f}, p={pval_t0:.6f} {'âœ…' if pval_t0 < 0.05 else 'âŒ'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   T+1 (next day):  r={corr_t1:+.4f}, p={pval_t1:.6f} {'âœ…' if pval_t1 < 0.05 else 'âŒ'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   T+2 (2 days out): r={corr_t2:+.4f}, p={pval_t2:.6f} {'âœ…' if pval_t2 < 0.05 else 'âŒ'}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "# Create DataFrame\n",
    "lagged_df = pd.DataFrame(lagged_correlations)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"ğŸ“Š Lagged Correlation Summary:\")\n",
    "print(f\"   Mean T+0 correlation: {lagged_df['corr_t0'].mean():.4f}\")\n",
    "print(f\"   Mean T+1 correlation: {lagged_df['corr_t1'].mean():.4f}\")\n",
    "print(f\"   Mean T+2 correlation: {lagged_df['corr_t2'].mean():.4f}\")\n",
    "print(f\"\\n   Significant T+0: {(lagged_df['pval_t0'] < 0.05).sum()} / {len(lagged_df)}\")\n",
    "print(f\"   Significant T+1: {(lagged_df['pval_t1'] < 0.05).sum()} / {len(lagged_df)}\")\n",
    "print(f\"   Significant T+2: {(lagged_df['pval_t2'] < 0.05).sum()} / {len(lagged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cf6d1",
   "metadata": {},
   "source": [
    "## 10. Visualize Correlation Results\n",
    "\n",
    "Create comprehensive visualizations showing sentiment-return relationships: scatter plots, correlation heatmaps, and time series comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbefed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Scatter Plot: Sentiment vs Returns (All Stocks)\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for symbol in sorted(clean_data[\"stock\"].unique()):\n",
    "    stock_data = clean_data[clean_data[\"stock\"] == symbol]\n",
    "    ax.scatter(\n",
    "        stock_data[\"avg_sentiment\"],\n",
    "        stock_data[\"daily_return\"],\n",
    "        alpha=0.5,\n",
    "        s=30,\n",
    "        label=symbol,\n",
    "    )\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(clean_data[\"avg_sentiment\"], clean_data[\"daily_return\"], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(\n",
    "    clean_data[\"avg_sentiment\"].min(), clean_data[\"avg_sentiment\"].max(), 100\n",
    ")\n",
    "ax.plot(\n",
    "    x_line,\n",
    "    p(x_line),\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    alpha=0.8,\n",
    "    label=f\"Trend Line (r={overall_corr:.3f})\",\n",
    ")\n",
    "\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5, alpha=0.5)\n",
    "ax.axvline(x=0, color=\"black\", linestyle=\"-\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Daily Average Sentiment Score\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Daily Stock Return (%)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Sentiment vs Stock Returns Correlation\\nPearson r={overall_corr:.4f}, p={overall_pval:.6f}\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(loc=\"best\", fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation Heatmap: Per-Stock Correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = correlations_df.set_index(\"stock\")[[\"correlation\"]].T\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\".4f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    center=0,\n",
    "    vmin=-0.5,\n",
    "    vmax=0.5,\n",
    "    cbar_kws={\"label\": \"Pearson Correlation\"},\n",
    "    linewidths=2,\n",
    "    linecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Sentiment-Return Correlation by Stock\\n(Darker Green = Stronger Positive Correlation)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Correlation Heatmap Interpretation:\")\n",
    "print(\"   Green: Positive correlation (sentiment â†‘ â†’ returns â†‘)\")\n",
    "print(\"   Red: Negative correlation (sentiment â†‘ â†’ returns â†“)\")\n",
    "print(\"   Yellow: Weak/No correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lagged Correlation Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "x = np.arange(len(lagged_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(\n",
    "    x - width,\n",
    "    lagged_df[\"corr_t0\"],\n",
    "    width,\n",
    "    label=\"T+0 (Same Day)\",\n",
    "    alpha=0.8,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax.bar(x, lagged_df[\"corr_t1\"], width, label=\"T+1 (Next Day)\", alpha=0.8, color=\"green\")\n",
    "ax.bar(\n",
    "    x + width,\n",
    "    lagged_df[\"corr_t2\"],\n",
    "    width,\n",
    "    label=\"T+2 (2 Days Out)\",\n",
    "    alpha=0.8,\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "ax.set_xlabel(\"Stock Symbol\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Pearson Correlation Coefficient\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Lagged Sentiment-Return Correlations\\n(Testing Predictive Power)\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(lagged_df[\"stock\"])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Lagged Correlation Insights:\")\n",
    "print(\"   T+0: Sentiment and returns on same day (contemporaneous)\")\n",
    "print(\"   T+1: Today's sentiment predicting tomorrow's return (1-day lag)\")\n",
    "print(\"   T+2: Today's sentiment predicting day-after return (2-day lag)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Time Series: Sentiment and Returns Over Time (Example: AAPL)\n",
    "example_stock = \"AAPL\"\n",
    "stock_ts = (\n",
    "    clean_data[clean_data[\"stock\"] == example_stock].sort_values(\"trading_date\").copy()\n",
    ")\n",
    "\n",
    "# Calculate rolling averages for smoothing\n",
    "stock_ts[\"sentiment_ma\"] = (\n",
    "    stock_ts[\"avg_sentiment\"].rolling(window=20, center=True).mean()\n",
    ")\n",
    "stock_ts[\"return_ma\"] = stock_ts[\"daily_return\"].rolling(window=20, center=True).mean()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Plot 1: Sentiment over time\n",
    "axes[0].plot(\n",
    "    stock_ts[\"trading_date\"],\n",
    "    stock_ts[\"avg_sentiment\"],\n",
    "    alpha=0.3,\n",
    "    color=\"blue\",\n",
    "    linewidth=0.5,\n",
    "    label=\"Daily Sentiment\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    stock_ts[\"trading_date\"],\n",
    "    stock_ts[\"sentiment_ma\"],\n",
    "    color=\"darkblue\",\n",
    "    linewidth=2,\n",
    "    label=\"20-Day MA Sentiment\",\n",
    ")\n",
    "axes[0].axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "axes[0].set_ylabel(\"Sentiment Score\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    f\"{example_stock}: Sentiment and Returns Time Series Analysis\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].legend(loc=\"best\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Returns over time\n",
    "axes[1].plot(\n",
    "    stock_ts[\"trading_date\"],\n",
    "    stock_ts[\"daily_return\"],\n",
    "    alpha=0.3,\n",
    "    color=\"green\",\n",
    "    linewidth=0.5,\n",
    "    label=\"Daily Return\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    stock_ts[\"trading_date\"],\n",
    "    stock_ts[\"return_ma\"],\n",
    "    color=\"darkgreen\",\n",
    "    linewidth=2,\n",
    "    label=\"20-Day MA Return\",\n",
    ")\n",
    "axes[1].axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel(\"Date\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Daily Return (%)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend(loc=\"best\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ“Š Time Series Visualization for {example_stock}:\")\n",
    "print(\"   Compare visual patterns between sentiment (top) and returns (bottom)\")\n",
    "print(\"   Look for periods where sentiment and returns move together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858885f",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance Testing\n",
    "\n",
    "Perform comprehensive statistical tests to validate correlation significance and calculate confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance summary\n",
    "print(\"ğŸ”¬ Statistical Significance Analysis\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall significance\n",
    "print(f\"ğŸ“Š OVERALL CORRELATION (All Stocks Combined):\")\n",
    "print(f\"   Pearson r: {overall_corr:+.4f}\")\n",
    "print(f\"   P-value: {overall_pval:.8f}\")\n",
    "print(f\"   Sample size: {len(clean_data):,} observations\")\n",
    "print(f\"   Significance level: Î± = 0.05\")\n",
    "print(\n",
    "    f\"   Result: {'âœ… STATISTICALLY SIGNIFICANT' if overall_pval < 0.05 else 'âŒ NOT SIGNIFICANT'}\"\n",
    ")\n",
    "\n",
    "# Calculate confidence interval for overall correlation\n",
    "n = len(clean_data)\n",
    "fisher_z = np.arctanh(overall_corr)\n",
    "se = 1 / np.sqrt(n - 3)\n",
    "ci_lower_z = fisher_z - 1.96 * se\n",
    "ci_upper_z = fisher_z + 1.96 * se\n",
    "ci_lower = np.tanh(ci_lower_z)\n",
    "ci_upper = np.tanh(ci_upper_z)\n",
    "print(f\"   95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ“ˆ PER-STOCK SIGNIFICANCE TESTING:\")\n",
    "print(f\"   Stocks tested: {len(correlations_df)}\")\n",
    "print(f\"   Significant correlations (p < 0.05): {correlations_df['significant'].sum()}\")\n",
    "print(\n",
    "    f\"   Significance rate: {correlations_df['significant'].sum() / len(correlations_df) * 100:.1f}%\"\n",
    ")\n",
    "\n",
    "# Power analysis (effect size)\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ” EFFECT SIZE ANALYSIS (Cohen's Guidelines):\")\n",
    "print(f\"   Small effect: |r| â‰¥ 0.10\")\n",
    "print(f\"   Medium effect: |r| â‰¥ 0.30\")\n",
    "print(f\"   Large effect: |r| â‰¥ 0.50\")\n",
    "print(f\"\\n   Overall correlation |r| = {abs(overall_corr):.4f}:\")\n",
    "if abs(overall_corr) >= 0.50:\n",
    "    print(f\"   âœ… LARGE effect size\")\n",
    "elif abs(overall_corr) >= 0.30:\n",
    "    print(f\"   âœ… MEDIUM effect size\")\n",
    "elif abs(overall_corr) >= 0.10:\n",
    "    print(f\"   âœ… SMALL effect size\")\n",
    "else:\n",
    "    print(f\"   âŒ Negligible effect size\")\n",
    "\n",
    "# Stock-level effect sizes\n",
    "print(f\"\\n   Per-Stock Effect Sizes:\")\n",
    "for _, row in correlations_df.iterrows():\n",
    "    abs_corr = abs(row[\"correlation\"])\n",
    "    if abs_corr >= 0.50:\n",
    "        effect = \"LARGE\"\n",
    "    elif abs_corr >= 0.30:\n",
    "        effect = \"MEDIUM\"\n",
    "    elif abs_corr >= 0.10:\n",
    "        effect = \"SMALL\"\n",
    "    else:\n",
    "        effect = \"Negligible\"\n",
    "    print(f\"   {row['stock']}: |r|={abs_corr:.4f} â†’ {effect}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ“‹ INTERPRETATION SUMMARY:\")\n",
    "print(\n",
    "    f\"   â€¢ Correlation strength: {'Weak' if abs(overall_corr) < 0.3 else 'Moderate' if abs(overall_corr) < 0.5 else 'Strong'}\"\n",
    ")\n",
    "print(\n",
    "    f\"   â€¢ Direction: {'Positive (sentiment â†‘ â†’ returns â†‘)' if overall_corr > 0 else 'Negative (sentiment â†‘ â†’ returns â†“)'}\"\n",
    ")\n",
    "print(\n",
    "    f\"   â€¢ Statistical validity: {'High confidence (p < 0.05)' if overall_pval < 0.05 else 'Low confidence (p â‰¥ 0.05)'}\"\n",
    ")\n",
    "print(\n",
    "    f\"   â€¢ Practical significance: {'Actionable for trading strategies' if abs(overall_corr) >= 0.3 and overall_pval < 0.05 else 'Limited practical value'}\"\n",
    ")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a6f4d",
   "metadata": {},
   "source": [
    "## 12. Investment Insights and Recommendations\n",
    "\n",
    "Based on sentiment-return correlation analysis, develop actionable trading strategies and risk assessments for Nova Financial Solutions.\n",
    "\n",
    "### Key Findings Summary\n",
    "\n",
    "**Correlation Strength**: The analysis reveals [describe overall correlation] between news sentiment and stock returns across 6 major technology stocks.\n",
    "\n",
    "**Statistical Significance**: [Discuss p-values and confidence intervals - whether relationships are statistically meaningful]\n",
    "\n",
    "**Predictive Power**: Lagged correlation analysis shows [describe T+1, T+2 lag results] suggesting [predictive capability or lack thereof].\n",
    "\n",
    "**Stock-Specific Patterns**: Individual stocks exhibit [describe variation in correlations] with [name stocks] showing strongest sentiment-return relationships.\n",
    "\n",
    "### Trading Strategy Recommendations\n",
    "\n",
    "1. **Sentiment-Based Entry Signals**:\n",
    "   - For stocks with positive correlations (r > 0.30, p < 0.05): Consider long positions when daily sentiment exceeds +0.2\n",
    "   - For stocks with negative correlations: Consider contrarian strategies\n",
    "\n",
    "2. **Risk Management**:\n",
    "   - Correlation strength varies by stock â†’ diversification essential\n",
    "   - Sentiment signals work best when combined with technical indicators\n",
    "   - Stop-loss recommendations: [based on volatility analysis]\n",
    "\n",
    "3. **Timing Considerations**:\n",
    "   - Lagged correlations suggest [optimal entry timing based on T+1, T+2 results]\n",
    "   - News volume matters: higher article counts â†’ stronger signal reliability\n",
    "\n",
    "### Limitations and Caveats\n",
    "\n",
    "- **Correlation â‰  Causation**: Sentiment may reflect rather than predict price movements\n",
    "- **Market Regime Dependency**: Results may vary in bull vs bear markets\n",
    "- **Publisher Bias**: Consider news source concentration from Task 1 EDA\n",
    "- **Sample Period**: Analysis covers [date range] - may not generalize to all market conditions\n",
    "\n",
    "### Future Research Directions\n",
    "\n",
    "- Incorporate technical indicators with sentiment for hybrid models\n",
    "- Test intraday sentiment vs returns correlation\n",
    "- Analyze sentiment-volatility relationships\n",
    "- Develop machine learning models for return prediction using sentiment features\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**: Sentiment analysis provides [weak/moderate/strong] predictive signals for stock returns. While statistically [significant/insignificant], the [small/medium/large] effect size suggests [actionable/limited] practical value for algorithmic trading strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
